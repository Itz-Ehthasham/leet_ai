ğŸš€ Leety AI â€“ Local AI Coding Assistant (Browser Extension)

Leety AI is a powerful developer-focused browser extension that brings an AI coding assistant directly into your browser. It is designed to help with debugging, code explanations, error fixing, and logic assistance â€” all while keeping your data 100% local and private.

Unlike cloud-based AI tools, Leety AI runs entirely on your local machine using a locally hosted LLM.

Read the Documentation of the Repo and Ollama 

âœ¨ Key Features

ğŸ§  Local AI Processing â€“ No data sent to external servers

âš¡ Instant Responses â€“ Low latency with local inference

ğŸ§© Code Debugging & Fix Suggestions

ğŸ“ Code Explanation & Optimization Help

ğŸ–±ï¸ In-page Popup UI â€“ Lightweight, fast, and distraction-free

ğŸ” Privacy First â€“ Your code never leaves your system

ğŸ§  AI Model Used

Leety AI uses the Ollama LLM runtime, which allows large language models to run locally on your system.

âœ… Model Requirement

Model: llama3.1:latest

Runtime: Ollama

Execution: Fully local (no internet required after setup)

ğŸ“¦ Prerequisites

Before running the extension, make sure you have:

Node.js (v18 or higher recommended)

Ollama installed locally

A system capable of running local LLMs (8GB RAM minimum recommended)

ğŸ”§ Installation Guide
1ï¸âƒ£ Install Ollama

Download and install Ollama from the official website:

ğŸ‘‰ https://ollama.com

Verify installation:

ollama --version

2ï¸âƒ£ Download the Required Model

Pull the latest LLaMA model locally:

ollama pull llama3.1:latest


Run the model once to ensure it works:

ollama run llama3.1:latest

3ï¸âƒ£ Start Ollama Server

Make sure Ollama is running in the background:

ollama serve


âš ï¸ Important:
Ollama runs on http://localhost:11434 by default.
The extension communicates with this local server.

4ï¸âƒ£ Allow Browser Extension Access (Important)

For Chrome-based extensions, set this environment variable before running Ollama:

Windows (PowerShell)
$env:OLLAMA_ORIGINS="chrome-extension://*"
ollama serve

macOS / Linux
export OLLAMA_ORIGINS="chrome-extension://*"
ollama serve

5ï¸âƒ£ Install the Extension

Open Chrome â†’ chrome://extensions

Enable Developer Mode

Click Load unpacked

Select the extension project folder

ğŸ–¥ï¸ How It Works

User types a prompt or sends code from the extension popup

The request is sent to the local Ollama server

llama3.1 processes the request locally

The response is streamed back and rendered in the UI

ğŸ“Œ No cloud calls. No API keys. No tracking.

ğŸ› ï¸ Tech Stack

Frontend: React + TypeScript + Tailwind CSS

Browser APIs: Chrome Extension APIs (Manifest v3)

LLM Runtime: Ollama

Model: LLaMA 3.1 (Latest)

ğŸ”’ Privacy & Security

All AI inference runs locally

No prompts or responses are logged externally

No third-party analytics or tracking

Works fully offline after setup

âš ï¸ Known Limitations

Requires a system capable of running local LLMs

First response may be slightly slower due to model warm-up

Performance depends on CPU/GPU availability

ğŸš§ Future Enhancements

Streaming token-by-token responses

Model selection from UI

Code context awareness (file-level analysis)

Multi-tab memory support

ğŸ¤ Contribution

Contributions are welcome!
Feel free to fork the repo, submit PRs, or suggest improvements.

ğŸ“„ License

This project is licensed under the MIT License.

â­ Final Note

Leety AI is built for developers who care about speed, privacy, and control.
If you believe AI should run on your machine, this extension is for you.

If you want, I can also:

ğŸ”¥ Make this more startup-style

ğŸ§‘â€ğŸ’» Rewrite it for recruiters

ğŸ“¦ Add a Setup Troubleshooting section

ğŸ¯ Optimize it for GitHub stars

Just tell me ğŸ‘
